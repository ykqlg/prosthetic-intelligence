{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import scipy.signal as signal\n",
    "from scipy.signal import find_peaks\n",
    "import shutil\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = signal.butter(order, cutoff, fs=fs, btype='low', analog=False)\n",
    "    y = signal.lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def butter_highpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = signal.butter(order, cutoff,fs=fs, btype='high', analog=False)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag_elements: ['output/20240104_091258.csv', 'output/20240104_091511.csv', 'output/20240104_094232.csv', 'output/20240104_094302.csv', 'output/20240104_094325.csv', 'output/20240104_094355.csv', 'output/20240104_094428.csv', 'output/20240104_094500.csv', 'output/20240104_094524.csv', 'output/20240104_094552.csv', 'output/20240104_095025.csv', 'output/20240104_095103.csv', 'output/20240104_095131.csv', 'output/20240104_095247.csv', 'output/20240104_103510.csv', 'output/20240104_103726.csv', 'output/20240104_103910.csv', 'output/20240104_104247.csv', 'output/20240104_104319.csv', 'output/20240104_104408.csv', 'output/20240104_104623.csv', 'output/20240104_104652.csv', 'output/20240104_104717.csv', 'output/20240104_104832.csv', 'output/20240104_104902.csv', 'output/20240104_104948.csv', 'output/20240104_105015.csv', 'output/20240104_105044.csv', 'output/20240104_105108.csv', 'output/20240104_105200.csv', 'output/20240104_105315.csv', 'output/20240104_151321.csv', 'output/20240104_151343.csv', 'output/20240104_151441.csv', 'output/20240104_151512.csv', 'output/20240104_151537.csv', 'output/20240104_151907.csv', 'output/20240104_151942.csv', 'output/20240112_140057.csv', 'output/20240112_140127.csv', 'output/20240112_140149.csv', 'output/20240112_140223.csv', 'output/20240112_140310.csv', 'output/20240112_140332.csv', 'output/20240112_140407.csv', 'output/20240112_140504.csv', 'output/20240112_140535.csv', 'output/20240112_140606.csv', 'output/20240112_140629.csv', 'output/20240112_140739.csv', 'output/20240112_140800.csv', 'output/20240112_140819.csv', 'output/20240112_140842.csv', 'output/20240112_140908.csv', 'output/20240112_140928.csv', 'output/20240112_141014.csv', 'output/20240112_141041.csv', 'output/20240112_141109.csv', 'output/20240112_141133.csv', 'output/20240112_141157.csv', 'output/20240112_141217.csv', 'output/20240112_141259.csv', 'output/20240112_141657.csv', 'output/20240112_141719.csv', 'output/20240112_141741.csv', 'output/20240112_142240.csv', 'output/20240112_142316.csv', 'output/20240112_142339.csv', 'output/20240112_142401.csv', 'output/20240112_142422.csv', 'output/20240112_142521.csv', 'output/20240112_142635.csv', 'output/20240112_142701.csv', 'output/20240112_142750.csv', 'output/20240112_142849.csv', 'output/20240112_142919.csv', 'output/20240112_142941.csv', 'output/20240112_143007.csv', 'output/20240112_143113.csv', 'output/20240112_143133.csv', 'output/20240112_143154.csv', 'output/20240112_143246.csv', 'output/20240112_143306.csv', 'output/20240112_143326.csv', 'output/20240112_143349.csv', 'output/20240112_143409.csv', 'output/20240112_143506.csv', 'output/20240112_143526.csv', 'output/20240112_143545.csv', 'output/20240112_143605.csv', 'output/20240112_143629.csv', 'output/20240112_143650.csv', 'output/20240112_143712.csv', 'output/20240112_143800.csv', 'output/20240112_143821.csv', 'output/20240112_143842.csv', 'output/20240112_143919.csv', 'output/20240112_143938.csv', 'output/20240112_143956.csv', 'output/20240112_144015.csv', 'output/20240112_144035.csv', 'output/20240112_201655.csv', 'output/20240112_201738.csv', 'output/20240112_201851.csv', 'output/20240112_201910.csv', 'output/20240112_201932.csv', 'output/20240112_202052.csv', 'output/20240112_202114.csv', 'output/20240112_202132.csv', 'output/20240112_202209.csv', 'output/20240112_202230.csv', 'output/20240112_202251.csv', 'output/20240112_202313.csv', 'output/20240112_202354.csv', 'output/20240112_202413.csv', 'output/20240112_202431.csv', 'output/20240112_202543.csv', 'output/20240112_202605.csv', 'output/20240112_202626.csv']\n",
      "folder_files: ['20240112_140149.csv', '20240112_202626.csv', '20240112_140535.csv', '20240104_091511.csv', '20240104_105200.csv', '20240112_140223.csv', '20240112_141657.csv', '20240112_143007.csv', '20240112_140739.csv', '20240112_141259.csv', '20240112_202230.csv', '20240112_140127.csv', '20240104_104247.csv', '20240112_201851.csv', '20240112_141109.csv', '20240112_143133.csv', '20240104_104948.csv', '20240112_143545.csv', '20240112_141741.csv', '20240104_104717.csv', '20240104_104623.csv', '20240104_104832.csv', '20240112_202431.csv', '20240112_141157.csv', '20240112_142240.csv', '20240112_143526.csv', '20240112_141133.csv', '20240104_094552.csv', '20240112_143349.csv', '20240112_142919.csv', '20240112_143842.csv', '20240112_143956.csv', '20240112_140504.csv', '20240112_202605.csv', '20240104_094500.csv', '20240112_142422.csv', '20240112_140908.csv', '20240112_143629.csv', '20240112_201655.csv', '20240112_140310.csv', '20240104_095025.csv', '20240112_140407.csv', '20240112_143113.csv', '20240112_144015.csv', '20240112_140606.csv', '20240112_202251.csv', '20240104_095247.csv', '20240104_095131.csv', '20240112_141719.csv', '20240104_091258.csv', '20240112_140629.csv', '20240112_143306.csv', '20240104_103510.csv', '20240112_143506.csv', '20240104_104319.csv', '20240112_202114.csv', '20240104_151321.csv', '20240112_202313.csv', '20240104_105044.csv', '20240104_095103.csv', '20240112_143409.csv', '20240104_103910.csv', '20240104_104408.csv', '20240112_201738.csv', '20240104_103726.csv', '20240104_094232.csv', '20240112_140928.csv', '20240104_151441.csv', '20240112_143919.csv', '20240112_202052.csv', '20240104_105315.csv', '20240112_143605.csv', '20240112_143326.csv', '20240112_143800.csv', '20240112_142849.csv', '20240112_201932.csv', '20240112_143938.csv', '20240112_143821.csv', '20240112_142401.csv', '20240112_142750.csv', '20240112_141217.csv', '20240112_141041.csv', '20240112_144035.csv', '20240112_140842.csv', '20240112_142941.csv', '20240112_143712.csv', '20240112_140057.csv', '20240104_151942.csv', '20240112_140800.csv', '20240104_094428.csv', '20240104_094325.csv', '20240104_151343.csv', '20240104_094302.csv', '20240112_202543.csv', '20240104_151537.csv', '20240104_105108.csv', '20240112_143246.csv', '20240112_142521.csv', '20240112_202354.csv', '20240112_142316.csv', '20240104_105015.csv', '20240112_201910.csv', '20240104_094524.csv', '20240112_142635.csv', '20240112_140332.csv', '20240112_141014.csv', '20240112_202413.csv', '20240104_151907.csv', '20240112_140819.csv', '20240112_142701.csv', '20240112_143650.csv', '20240112_143154.csv', '20240112_142339.csv', '20240112_202132.csv', '20240104_151512.csv', '20240104_094355.csv', '20240112_202209.csv', '20240104_104652.csv', '20240104_104902.csv']\n",
      "count: 0\n"
     ]
    }
   ],
   "source": [
    "# 根据标签文件的内容，清除./output文件夹中的无关文件\n",
    "def remove_files_not_in_label(tag_file, folder_path):\n",
    "    # 读取标签文件的第一列元素\n",
    "    with open(tag_file, 'r') as tag_csv:\n",
    "        tag_reader = csv.reader(tag_csv)\n",
    "        # 跳过第一行（列名称）\n",
    "        next(tag_reader)\n",
    "        tag_elements = [row[0] for row in tag_reader]\n",
    "\n",
    "    # 获取文件夹中的所有文件名\n",
    "    folder_files = os.listdir(folder_path)\n",
    "    print('tag_elements:',tag_elements)\n",
    "    print('folder_files:',folder_files)\n",
    "\n",
    "    count = 0\n",
    "    # 遍历文件夹中的文件名\n",
    "    for filename in folder_files:\n",
    "        # 检查文件名是否在标签文件的第一列元素中\n",
    "        if not any(filename in tag for tag in tag_elements):\n",
    "            # 构造文件的完整路径\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            # 删除文件\n",
    "            os.remove(file_path)\n",
    "            # print(f\"File '{file_path}' removed.\")\n",
    "            count += 1\n",
    "    \n",
    "    print('count:',count)\n",
    "\n",
    "tag_file_path = './label_file.csv'\n",
    "folder_path = './output/'\n",
    "remove_files_not_in_label(tag_file_path, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1], dtype=int64), array([51, 68]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看当前采集数据的标签分布情况\n",
    "\n",
    "df = pd.read_csv('./label_file.csv')\n",
    "labels = df['Label']\n",
    "np.unique(labels,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./output_cropped/20240104_091258.csv has 2664 rows\n",
      "./output_cropped/20240104_091511.csv has 2647 rows\n",
      "./output_cropped/20240104_094232.csv has 2646 rows\n",
      "./output_cropped/20240104_094302.csv has 2678 rows\n",
      "./output_cropped/20240104_094325.csv has 2668 rows\n",
      "./output_cropped/20240104_094355.csv has 2662 rows\n",
      "./output_cropped/20240104_094428.csv has 2660 rows\n",
      "./output_cropped/20240104_094500.csv has 2636 rows\n",
      "./output_cropped/20240104_094524.csv has 2671 rows\n",
      "./output_cropped/20240104_094552.csv has 2662 rows\n",
      "./output_cropped/20240104_095025.csv has 2650 rows\n",
      "./output_cropped/20240104_095103.csv has 2654 rows\n",
      "./output_cropped/20240104_095131.csv has 2662 rows\n",
      "./output_cropped/20240104_095247.csv has 2644 rows\n",
      "./output_cropped/20240104_103510.csv has 2634 rows\n",
      "./output_cropped/20240104_103726.csv has 2675 rows\n",
      "./output_cropped/20240104_103910.csv has 2650 rows\n",
      "./output_cropped/20240104_104247.csv has 2660 rows\n",
      "./output_cropped/20240104_104319.csv has 2652 rows\n",
      "./output_cropped/20240104_104408.csv has 2663 rows\n",
      "./output_cropped/20240104_104623.csv has 2679 rows\n",
      "./output_cropped/20240104_104652.csv has 2684 rows\n",
      "./output_cropped/20240104_104717.csv has 2672 rows\n",
      "./output_cropped/20240104_104832.csv has 2680 rows\n",
      "./output_cropped/20240104_104902.csv has 2646 rows\n",
      "./output_cropped/20240104_104948.csv has 2658 rows\n",
      "./output_cropped/20240104_105015.csv has 2663 rows\n",
      "./output_cropped/20240104_105044.csv has 2656 rows\n",
      "./output_cropped/20240104_105108.csv has 2666 rows\n",
      "./output_cropped/20240104_105200.csv has 2643 rows\n",
      "./output_cropped/20240104_105315.csv has 2651 rows\n",
      "./output_cropped/20240104_151321.csv has 2658 rows\n",
      "./output_cropped/20240104_151343.csv has 2656 rows\n",
      "./output_cropped/20240104_151441.csv has 2640 rows\n",
      "./output_cropped/20240104_151512.csv has 2660 rows\n",
      "./output_cropped/20240104_151537.csv has 2660 rows\n",
      "./output_cropped/20240104_151907.csv has 2684 rows\n",
      "./output_cropped/20240104_151942.csv has 2687 rows\n",
      "./output_cropped/20240112_140057.csv has 2660 rows\n",
      "./output_cropped/20240112_140127.csv has 2675 rows\n",
      "./output_cropped/20240112_140149.csv has 2658 rows\n",
      "./output_cropped/20240112_140223.csv has 2667 rows\n",
      "./output_cropped/20240112_140310.csv has 2660 rows\n",
      "./output_cropped/20240112_140332.csv has 2667 rows\n",
      "./output_cropped/20240112_140407.csv has 2674 rows\n",
      "./output_cropped/20240112_140504.csv has 2644 rows\n",
      "./output_cropped/20240112_140535.csv has 2674 rows\n",
      "./output_cropped/20240112_140606.csv has 2662 rows\n",
      "./output_cropped/20240112_140629.csv has 2664 rows\n",
      "./output_cropped/20240112_140739.csv has 2655 rows\n",
      "./output_cropped/20240112_140800.csv has 2687 rows\n",
      "./output_cropped/20240112_140819.csv has 2638 rows\n",
      "./output_cropped/20240112_140842.csv has 2688 rows\n",
      "./output_cropped/20240112_140908.csv has 2686 rows\n",
      "./output_cropped/20240112_140928.csv has 2656 rows\n",
      "./output_cropped/20240112_141014.csv has 2684 rows\n",
      "./output_cropped/20240112_141041.csv has 2676 rows\n",
      "./output_cropped/20240112_141109.csv has 2684 rows\n",
      "./output_cropped/20240112_141133.csv has 2675 rows\n",
      "./output_cropped/20240112_141157.csv has 2672 rows\n",
      "./output_cropped/20240112_141217.csv has 2683 rows\n",
      "./output_cropped/20240112_141259.csv has 2668 rows\n",
      "./output_cropped/20240112_141657.csv has 2662 rows\n",
      "./output_cropped/20240112_141719.csv has 2683 rows\n",
      "./output_cropped/20240112_141741.csv has 2658 rows\n",
      "./output_cropped/20240112_142240.csv has 2650 rows\n",
      "./output_cropped/20240112_142316.csv has 2648 rows\n",
      "./output_cropped/20240112_142339.csv has 2670 rows\n",
      "./output_cropped/20240112_142401.csv has 2660 rows\n",
      "./output_cropped/20240112_142422.csv has 2660 rows\n",
      "./output_cropped/20240112_142521.csv has 2652 rows\n",
      "./output_cropped/20240112_142635.csv has 2682 rows\n",
      "./output_cropped/20240112_142701.csv has 2663 rows\n",
      "./output_cropped/20240112_142750.csv has 2659 rows\n",
      "./output_cropped/20240112_142849.csv has 2658 rows\n",
      "./output_cropped/20240112_142919.csv has 2683 rows\n",
      "./output_cropped/20240112_142941.csv has 2662 rows\n",
      "./output_cropped/20240112_143007.csv has 2662 rows\n",
      "./output_cropped/20240112_143113.csv has 2687 rows\n",
      "./output_cropped/20240112_143133.csv has 2662 rows\n",
      "./output_cropped/20240112_143154.csv has 2662 rows\n",
      "./output_cropped/20240112_143246.csv has 2675 rows\n",
      "./output_cropped/20240112_143306.csv has 2687 rows\n",
      "./output_cropped/20240112_143326.csv has 2667 rows\n",
      "./output_cropped/20240112_143349.csv has 2667 rows\n",
      "./output_cropped/20240112_143409.csv has 2690 rows\n",
      "./output_cropped/20240112_143506.csv has 2654 rows\n",
      "./output_cropped/20240112_143526.csv has 2643 rows\n",
      "./output_cropped/20240112_143545.csv has 2667 rows\n",
      "./output_cropped/20240112_143605.csv has 2667 rows\n",
      "./output_cropped/20240112_143629.csv has 2674 rows\n",
      "./output_cropped/20240112_143650.csv has 2680 rows\n",
      "./output_cropped/20240112_143712.csv has 2646 rows\n",
      "./output_cropped/20240112_143800.csv has 2659 rows\n",
      "./output_cropped/20240112_143821.csv has 2640 rows\n",
      "./output_cropped/20240112_143842.csv has 2670 rows\n",
      "./output_cropped/20240112_143919.csv has 2662 rows\n",
      "./output_cropped/20240112_143938.csv has 2656 rows\n",
      "./output_cropped/20240112_143956.csv has 2652 rows\n",
      "./output_cropped/20240112_144015.csv has 2652 rows\n",
      "./output_cropped/20240112_144035.csv has 2646 rows\n",
      "./output_cropped/20240112_201655.csv has 2652 rows\n",
      "./output_cropped/20240112_201738.csv has 2643 rows\n",
      "./output_cropped/20240112_201851.csv has 2631 rows\n",
      "./output_cropped/20240112_201910.csv has 2667 rows\n",
      "./output_cropped/20240112_201932.csv has 2636 rows\n",
      "./output_cropped/20240112_202052.csv has 2674 rows\n",
      "./output_cropped/20240112_202114.csv has 2667 rows\n",
      "./output_cropped/20240112_202132.csv has 2646 rows\n",
      "./output_cropped/20240112_202209.csv has 2667 rows\n",
      "./output_cropped/20240112_202230.csv has 2640 rows\n",
      "./output_cropped/20240112_202251.csv has 2631 rows\n",
      "./output_cropped/20240112_202313.csv has 2630 rows\n",
      "./output_cropped/20240112_202354.csv has 2634 rows\n",
      "./output_cropped/20240112_202413.csv has 2646 rows\n",
      "./output_cropped/20240112_202431.csv has 2666 rows\n",
      "./output_cropped/20240112_202543.csv has 2675 rows\n",
      "./output_cropped/20240112_202605.csv has 2682 rows\n",
      "./output_cropped/20240112_202626.csv has 2668 rows\n"
     ]
    }
   ],
   "source": [
    "# 数据清洗部分\n",
    "\n",
    "# 滤波绘制\n",
    "def plot_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    ACC_X = df['ACC_X']\n",
    "    ACC_Y = df['ACC_Y']\n",
    "    ACC_Z = df['ACC_Z']\n",
    "    time = df['Time']\n",
    "\n",
    "    # 滤波参数\n",
    "    fs = 1/np.mean(np.diff(time))\n",
    "    # plt.specgram(ACC_Z, NFFT=1024, Fs=fs, detrend=None)\n",
    "    cutoff_L,cutoff_H = 500,20\n",
    "    order_L,order_H = 6,5\n",
    "    truncate_length = 0\n",
    "    \n",
    "    ACC_X_filtered_L = butter_lowpass_filter(ACC_X, cutoff_L, fs, order_L)\n",
    "    ACC_X_filtered_H = butter_highpass_filter(ACC_X_filtered_L, cutoff_H, fs, order_H)[truncate_length:]\n",
    "    ACC_Y_filtered_L = butter_lowpass_filter(ACC_Y, cutoff_L, fs, order_L)\n",
    "    ACC_Y_filtered_H = butter_highpass_filter(ACC_Y_filtered_L, cutoff_H, fs, order_H)[truncate_length:]\n",
    "    ACC_Z_filtered_L = butter_lowpass_filter(ACC_Z, cutoff_L, fs, order_L)\n",
    "    ACC_Z_filtered_H = butter_highpass_filter(ACC_Z_filtered_L, cutoff_H, fs, order_H)[truncate_length:]\n",
    "    time = time.values[truncate_length:]\n",
    "\n",
    "    # 截取窗口\n",
    "    threshold = 1000 \n",
    "    peaks, _ = find_peaks(ACC_Z_filtered_H, height=threshold)\n",
    "    pivot = peaks[0]\n",
    "    start_index = pivot - int(0.4*fs)\n",
    "    end_index = pivot + int(1.4*fs)\n",
    "    # print(\"index:\",start_index,end_index)\n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(10, 10))\n",
    "    \n",
    "    axes[0].plot(time,ACC_X_filtered_H, label='ACC_X Data')\n",
    "    axes[1].plot(time,ACC_Y_filtered_H, label='ACC_Y Data')\n",
    "    axes[2].plot(time,ACC_Z_filtered_H, label='ACC_Z Data')\n",
    "    \n",
    "    for i, data_name in enumerate(['ACC_X', 'ACC_Y', 'ACC_Z']): \n",
    "        \n",
    "        axes[i].axvspan(time[start_index], time[end_index], facecolor='red', alpha=0.2, label='Target Waveform')\n",
    "        \n",
    "        # ax.set_title(data_name)\n",
    "        # axes[i].set_xlabel(\"Time (s)\")\n",
    "        axes[i].set_ylabel(data_name+' (mg)')\n",
    "        # axes[i].legend()\n",
    "    axes[0].set_xticks([])\n",
    "    axes[1].set_xticks([])\n",
    "    axes[2].set_xlabel(\"Time (s)\")\n",
    "        \n",
    "    plt.suptitle(filename)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return start_index,end_index\n",
    "\n",
    "# 裁剪目标波形\n",
    "def crop_data(filepath, newfoldername):\n",
    "    \n",
    "    newfilepath = os.path.join('.', newfoldername, os.path.basename(filepath))\n",
    "    # print(filepath)\n",
    "    # print(newfilepath)\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    ACC = df['ACC_Z']\n",
    "    time = df['Time']\n",
    "\n",
    "    fs = 1/np.mean(np.diff(time))\n",
    "    truncate_length = 0\n",
    "    \n",
    "\n",
    "    ACC_filtered_L = butter_lowpass_filter(ACC, cutoff=500, fs=fs, order=6)\n",
    "    ACC_filtered_H = butter_highpass_filter(ACC_filtered_L, cutoff=20, fs=fs, order=5)[truncate_length:]\n",
    "    time = time.values[truncate_length:]\n",
    "    \n",
    "    threshold = 1000 \n",
    "    peaks, _ = find_peaks(ACC_filtered_H, height=threshold)\n",
    "    pivot = peaks[0]\n",
    "    start_index = pivot - int(0.5*fs)\n",
    "    end_index = pivot + int(1.5*fs)\n",
    "    \n",
    "    df_cropped = df[start_index:end_index + 1]\n",
    "    df_cropped.to_csv(newfilepath, index=False)\n",
    "    return newfilepath\n",
    "   \n",
    "# 根据标签文件，裁剪目标波形\n",
    "def clean_data(labelfile, target_folder):\n",
    "    \n",
    "    df = pd.read_csv(labelfile)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        filepath = row.iloc[0]\n",
    "        # plot_data(filepath)\n",
    "        new_file_path = crop_data(filepath, target_folder)\n",
    "\n",
    "        # 检查裁剪后的文件的行数，避免出现错误数据\n",
    "        with open(new_file_path, 'r') as csv_file:\n",
    "            csv_reader = csv.reader(csv_file)\n",
    "        \n",
    "            row_count = len(list(csv_reader))\n",
    "            print(f\"{new_file_path} has {row_count} rows\")\n",
    "            \n",
    "    df['FilePath'] = df['FilePath'].apply(lambda x: x.replace('output', target_folder))\n",
    "    new_label_file_path = './label_file_replaced.csv'\n",
    "    df.to_csv(new_label_file_path, index=False)\n",
    "\n",
    "newfoldername = 'output_cropped'\n",
    "clean_data('./label_file.csv',newfoldername)\n",
    "\n",
    "# crop_data('./output/20240104_091258.csv', newfoldername)\n",
    "\n",
    "# plot_data('./output/20240104_091258.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征提取\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
